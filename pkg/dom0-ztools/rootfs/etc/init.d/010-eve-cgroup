#!/bin/sh

if test -f /proc/vmcore; then
    # NOOP if dump-capture kernel
    exit 0;
fi

hv=`cat /run/eve-hv-type`
default_cgroup_cpus_limit=1

default_cgroup_memory_limit=838860800 #800M
EVESERVICES="sshd eve-edgeview wwan lisp guacd pillar vtpm watchdog xen-tools newlogd memlogd memory-monitor debug monitor node-exporter vector"

#Increase memory limits temporarily for 'k' type only.
case $hv in
   k)
       default_cgroup_memory_limit=8388608000 #8G
       EVESERVICES="${EVESERVICES} kube"
       ;;
esac

dom0_cgroup_memory_soft_limit=$(</proc/cmdline grep -o '\bdom0_mem=[^, ]*' | cut -d = -f 2)
dom0_cgroup_memory_limit=$(</proc/cmdline grep -o "\bdom0_mem=[^,]*,max:[^ ]*" | cut -d : -f 2)
dom0_cgroup_cpus_limit=$(</proc/cmdline grep -o '\bdom0_max_vcpus=[^ ]*' | cut -d = -f 2)

eve_cgroup_memory_soft_limit=$(</proc/cmdline grep -o '\beve_mem=[^, ]*' | cut -d = -f 2)
eve_cgroup_memory_limit=$(</proc/cmdline grep -o "\beve_mem=[^,]*,max:[^ ]*" | cut -d : -f 2)
eve_cgroup_cpus_limit=$(</proc/cmdline grep -o '\beve_max_vcpus=[^ ]*' | cut -d = -f 2)

ctrd_cgroup_memory_soft_limit=$(</proc/cmdline grep -o '\bctrd_mem=[^, ]*' | cut -d = -f 2)
ctrd_cgroup_memory_limit=$(</proc/cmdline grep -o "\bctrd_mem=[^,]*,max:[^ ]*" | cut -d : -f 2)
ctrd_cgroup_cpus_limit=$(</proc/cmdline grep -o '\bctrd_max_vcpus=[^ ]*' | cut -d = -f 2)

if [ -z "${dom0_cgroup_memory_soft_limit}" ]; then
    echo "Setting default value of $default_cgroup_memory_limit for dom0_cgroup_memory_soft_limit"
    dom0_cgroup_memory_soft_limit=$default_cgroup_memory_limit
fi

if [ -z "${dom0_cgroup_memory_limit}" ]; then
    echo "Setting value of $dom0_cgroup_memory_soft_limit for dom0_cgroup_memory_limit"
    dom0_cgroup_memory_limit=$dom0_cgroup_memory_soft_limit
fi

if [ -z "${dom0_cgroup_cpus_limit}" ] || [ "${dom0_cgroup_cpus_limit}" = "0" ]; then
    echo "Setting default value of $default_cgroup_cpus_limit for dom0_cgroup_cpus_limit"
    dom0_cgroup_cpus_limit=$default_cgroup_cpus_limit
fi

if [ -z "${eve_cgroup_memory_soft_limit}" ]; then
    echo "Setting default value of $default_cgroup_memory_limit for eve_cgroup_memory_soft_limit"
    eve_cgroup_memory_soft_limit=$default_cgroup_memory_limit
fi

if [ -z "${eve_cgroup_memory_limit}" ]; then
    echo "Setting value of $eve_cgroup_memory_soft_limit for eve_cgroup_memory_limit"
    eve_cgroup_memory_limit=$eve_cgroup_memory_soft_limit
fi

if [ -z "${eve_cgroup_cpus_limit}" ] || [ "${eve_cgroup_cpus_limit}" = "0" ]; then
    echo "Setting default value of $default_cgroup_cpus_limit for eve_cgroup_cpus_limit"
    eve_cgroup_cpus_limit=$default_cgroup_cpus_limit
fi

if [ -z "${ctrd_cgroup_memory_soft_limit}" ]; then
    echo "Setting default value of $default_cgroup_memory_limit for ctrd_cgroup_memory_soft_limit"
    ctrd_cgroup_memory_soft_limit=$default_cgroup_memory_limit
fi

if [ -z "${ctrd_cgroup_memory_limit}" ]; then
    echo "Setting value of $ctrd_cgroup_memory_soft_limit for ctrd_cgroup_memory_limit"
    ctrd_cgroup_memory_limit=$ctrd_cgroup_memory_soft_limit
fi

if [ -z "${ctrd_cgroup_cpus_limit}" ] || [ "${ctrd_cgroup_cpus_limit}" = "0" ]; then
    echo "Setting default value of $default_cgroup_cpus_limit for ctrd_cgroup_cpus_limit"
    ctrd_cgroup_cpus_limit=$default_cgroup_cpus_limit
fi

CGROUPS="cpuset cpu cpuacct blkio memory devices freezer net_cls perf_event net_prio hugetlb pids systemd "

#Creating eve cgroup which will be parent/dom0 cgroup
for cg in $CGROUPS; do
    mkdir -p /sys/fs/cgroup/"${cg}"/eve
done

#Creating cgroup for individual eve services
for srv in $EVESERVICES; do
    for cg in $CGROUPS; do
        mkdir -p /sys/fs/cgroup/"${cg}"/eve/services/"${srv}"
    done
done

#Creating cgroup for containerd
mkdir -p /sys/fs/cgroup/memory/eve/containerd

existingCPULimit=$(</sys/fs/cgroup/cpuset/cpuset.cpus grep -o '0-[1-9]*' | cut -d '-' -f 2)
if [ -z "${existingCPULimit}" ]; then
    existingCPULimit=$(cat /sys/fs/cgroup/cpuset/cpuset.cpus)
fi

/bin/echo $dom0_cgroup_memory_limit > /sys/fs/cgroup/memory/eve/memory.limit_in_bytes
/bin/echo $dom0_cgroup_memory_soft_limit > /sys/fs/cgroup/memory/eve/memory.soft_limit_in_bytes
#Value that we are trying to update should not be greater than the existing value in cgroup system.
if [ -n "${existingCPULimit}" ] && [ "$existingCPULimit" -ge "$dom0_cgroup_cpus_limit" ]; then
    /bin/echo "0-$((dom0_cgroup_cpus_limit-1))" > /sys/fs/cgroup/cpuset/eve/cpuset.cpus
    /bin/echo "0-$((dom0_cgroup_cpus_limit-1))" > /sys/fs/cgroup/cpuset/eve/cpuset.mems
fi

/bin/echo $ctrd_cgroup_memory_limit > /sys/fs/cgroup/memory/eve/containerd/memory.limit_in_bytes
/bin/echo $ctrd_cgroup_memory_soft_limit > /sys/fs/cgroup/memory/eve/containerd/memory.soft_limit_in_bytes

/bin/echo $eve_cgroup_memory_limit > /sys/fs/cgroup/memory/eve/services/memory.limit_in_bytes
/bin/echo $eve_cgroup_memory_soft_limit > /sys/fs/cgroup/memory/eve/services/memory.soft_limit_in_bytes
if [ -n "${existingCPULimit}" ] && [ "$existingCPULimit" -ge "$eve_cgroup_cpus_limit" ]; then
    /bin/echo "0-$((eve_cgroup_cpus_limit-1))" > /sys/fs/cgroup/cpuset/eve/services/cpuset.cpus
    /bin/echo "0-$((eve_cgroup_cpus_limit-1))" > /sys/fs/cgroup/cpuset/eve/services/cpuset.mems
fi

for srv in $EVESERVICES; do
    /bin/echo $eve_cgroup_memory_limit > /sys/fs/cgroup/memory/eve/services/"${srv}"/memory.limit_in_bytes
    /bin/echo $eve_cgroup_memory_soft_limit > /sys/fs/cgroup/memory/eve/services/"${srv}"/memory.soft_limit_in_bytes
    if [ -n "${existingCPULimit}" ] && [ "$existingCPULimit" -ge "$eve_cgroup_cpus_limit" ]; then
        /bin/echo "0-$((eve_cgroup_cpus_limit-1))" > /sys/fs/cgroup/cpuset/eve/services/"${srv}"/cpuset.cpus
        /bin/echo "0-$((eve_cgroup_cpus_limit-1))" > /sys/fs/cgroup/cpuset/eve/services/"${srv}"/cpuset.mems
    fi
done

# Once all 'cpuset.cpus' and 'cpuset.mems' are prepared copy memlogd
# tasks from 'memory' controller to the 'cpuset' controller.
for pid in $(cat /sys/fs/cgroup/memory/eve/services/memlogd/tasks); do
    echo $pid > /sys/fs/cgroup/cpuset/eve/services/memlogd/tasks;
done

# Disable cgroup v1 RT bandwidth throttling globally.
#
# By default the kernel sets sched_rt_runtime_us=950000 which means the
# RT bandwidth check is enabled: every cgroup's cpu.rt_runtime_us must be
# positive AND the sum of children's values <= parent's value.  This makes
# it impossible to run multiple RT containers without splitting the budget
# â€” even when they are pinned to non-overlapping CPUs.
#
# Setting the sysctl to -1 disables the bandwidth check entirely
# (rt_bandwidth_enabled() returns false).  sched_setscheduler(SCHED_FIFO)
# then succeeds regardless of the per-cgroup cpu.rt_runtime_us value.
#
# This is safe because RT containers are always pinned to isolated CPUs
# via cpuset, so they cannot starve the rest of the system.
/sbin/sysctl -w kernel.sched_rt_runtime_us=-1 2>/dev/null || true
